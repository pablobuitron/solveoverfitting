# Enhanced model configuration from optimization report
# Increased capacity with better regularization to prevent overfitting
'hidden_size': 256 # 128 -> 256 (why Samer?)
'lstm_layers': 2  # Reduced 3 -> 2 to prevent overfitting
'attention_head_size': 16  # Increased 8 -> 16 for better attention
'hidden_continuous_size': 128  # Increased 64 -> 128 for better feature processing
'output_size': 7  # For 7 quantiles (0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95) # TODO |> implementare